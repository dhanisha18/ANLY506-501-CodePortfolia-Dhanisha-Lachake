---
title: "Week10"
author: "Dhanisha"
date: "July 28, 2019"
output: html_document
---	

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#The K-means clustering algorithm is another very important algorithm in high dimensional data analysis that has gone back many decades now.
#The K-means approach, like many clustering methods, is highly algorithmic and is iterative. The basic idea is that we have to try to find the centroids of a fixed number of clusters of points in a high-dimensional space. In two dimensions,  there are a bunch of clouds of points on the plane and you want to figure out where the centers of each one of those clouds is.
#In two dimensions, we could probably just look at the data and figure out with a high degree of accuracy where the cluster centroids are. But what if the data are in a 100-dimensional space? That’s where we need an algorithm.
#The K-means approach is a partitioning way, where in the data are partitioned into groups at each iteration of the algorithm. The only pre-requirement is that we have to  pre-specify how many clusters and this is not the best method as we may not be aware of it in advance but we canalways change that later

#The outline of the algorithm is: 
#Fix the number of clusters at some integer greater than or equal to 2
#Start with the “centroids” of each cluster; initially you might just pick a random set of points as the centroids
#Assign points to their closest centroid; cluster membership corresponds to the centroid assignment
#Reclaculate centroid positions and repeat.

```{r}
#Illustrating the K-means algorithm
#We will use an example with simulated data to demonstrate how the K-means algorithm works. Here we simulate some data from three clusters and plot the dataset below.
# Set the seed
set.seed(1234)

# Set the X & Y vectors
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)

# Create the Plot
plot(x, y, col = "pink", pch = 19, cex = 2)

# Set the text in the plot
text(x + 0.05, y + 0.05, labels = as.character(1:12))

#Using the kmeans() function

#The kmeans() function in R implements the K-means algorithm and can be found in the stats package, which comes with R and is usually already loaded when we start R.
#Here the Two key parameters that we  specify are x, which is a matrix or data frame of data, and centers which is either an integer indicating the number of clusters or a matrix indicating the locations of the initial cluster centroids.Here each row is an observation and each column is a variable of that observation.

dataFrame <- data.frame(x, y)
KMeansOBject <- kmeans(dataFrame, centers = 3)
names(KMeansOBject)
dataFrame <- data.frame(x, y)
KMeansOBject <- kmeans(dataFrame, centers = 3)
names(KMeansOBject)

#You can see which cluster each data point got assigned to by looking at the cluster element of the list returned by the kmeans() function.
# Check assigned cluster for data point
KMeansOBject$cluster

#Building heatmaps from K-means solutions
#A heat map or image plot is sometimes a useful way to visualize matrix or array data. The idea is that each cell of the image is colored in a manner proportional to the value in the corresponding matrix element. It take a bit of work to get this to look right in R but the result can be very useful, especially for high-dimensional datasets that can be visualized using the simple plots we used above.

#First, we need to find the K-means solution:

set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
KMeansOBject <- kmeans(dataMatrix, centers = 3)
#Then we can make an image plot using the K-means clusters:
par(mfrow = c(1, 2))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n", main = "OG Data")
image(t(dataMatrix)[, order(KMeansOBject$cluster)], yaxt = "n", main = "Clustered Data")

```




```{r, echo=FALSE}

```
